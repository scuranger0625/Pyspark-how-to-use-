{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ebf0d78",
   "metadata": {},
   "source": [
    "# PySpark 函式與用法示例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63002d24",
   "metadata": {},
   "source": [
    "## 1. SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e950b131",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 初始化 SparkSession，這是 PySpark 的入口點\n",
    "spark = SparkSession.builder.appName(\"範例應用\").getOrCreate()\n",
    "\n",
    "# 獲取 SparkContext（舊版入口點）\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# 停止 SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f26a812",
   "metadata": {},
   "source": [
    "## 2. DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ef86a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 建立 DataFrame\n",
    "data = [(1, \"Alice\", 29), (2, \"Bob\", 31)]  # 範例數據\n",
    "columns = [\"id\", \"name\", \"age\"]            # 欄位名稱\n",
    "df = spark.createDataFrame(data, columns)  # 建立 DataFrame\n",
    "\n",
    "# 顯示數據\n",
    "df.show()                 # 顯示前 20 行數據\n",
    "df.printSchema()          # 顯示數據結構\n",
    "df.columns                # 獲取所有列名\n",
    "df.dtypes                 # 獲取列名與資料型別\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3536953b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 過濾數據，篩選年齡大於 30 的資料\n",
    "filtered_df = df.filter(df.age > 30)\n",
    "\n",
    "# 選取特定欄位\n",
    "selected_df = df.select(\"name\", \"age\")\n",
    "\n",
    "# 排序數據，根據年齡降序排序\n",
    "sorted_df = df.orderBy(\"age\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc4ac0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import avg, count\n",
    "\n",
    "# 分組計算：計算每個年齡的數量\n",
    "df.groupBy(\"age\").count().show()\n",
    "\n",
    "# 聚合函式：計算年齡的平均值\n",
    "df.agg(avg(\"age\").alias(\"平均年齡\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c059c87",
   "metadata": {},
   "source": [
    "## 3. SQL 操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d603128",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 建立臨時表\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# 執行 SQL 查詢\n",
    "result = spark.sql(\"SELECT name, age FROM people WHERE age > 30\")\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6355462",
   "metadata": {},
   "source": [
    "## 4. RDD（彈性分布式數據集）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1e96f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 創建 RDD\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "\n",
    "# 使用 Map 與 Filter 函式\n",
    "mapped_rdd = rdd.map(lambda x: x * 2)  # 將每個元素乘以 2\n",
    "filtered_rdd = rdd.filter(lambda x: x > 3)  # 過濾大於 3 的元素\n",
    "\n",
    "# 收集結果並打印\n",
    "collected = filtered_rdd.collect()\n",
    "print(collected)  # [4, 5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189bfb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 使用 Reduce 函式計算總和\n",
    "sum_rdd = rdd.reduce(lambda x, y: x + y)\n",
    "\n",
    "# 基本聚合操作\n",
    "count = rdd.count()       # 計算元素個數\n",
    "max_value = rdd.max()     # 找出最大值\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53b84bf",
   "metadata": {},
   "source": [
    "## 5. DataFrame 函式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fa72bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import col, lit, when\n",
    "\n",
    "# 新增欄位，將年齡乘以 2\n",
    "df = df.withColumn(\"新欄位\", df.age * 2)\n",
    "\n",
    "# 條件運算，新增欄位判斷是否成年\n",
    "df = df.withColumn(\"是否成年\", when(col(\"age\") >= 18, lit(True)).otherwise(lit(False)))\n",
    "\n",
    "# 字符串處理，將名稱轉為大寫\n",
    "from pyspark.sql.functions import upper, lower\n",
    "df = df.withColumn(\"名稱大寫\", upper(col(\"name\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f4e7ec",
   "metadata": {},
   "source": [
    "## 6. MLlib（機器學習庫）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793b3845",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# 創建特徵向量\n",
    "assembler = VectorAssembler(inputCols=[\"age\"], outputCol=\"features\")\n",
    "data = assembler.transform(df)\n",
    "\n",
    "# 建立線性回歸模型並訓練\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"age\")\n",
    "model = lr.fit(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b12eb9",
   "metadata": {},
   "source": [
    "## 7. Streaming（數據流處理）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c870b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# 創建 StreamingContext，每秒處理一次數據\n",
    "ssc = StreamingContext(sc, 1)\n",
    "\n",
    "# 創建數據流\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "\n",
    "# 數據處理：統計單詞出現次數\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "word_counts = words.map(lambda word: (word, 1)).reduceByKey(lambda x, y: x + y)\n",
    "word_counts.pprint()\n",
    "\n",
    "# 啟動數據流\n",
    "ssc.start()\n",
    "ssc.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d8d09a",
   "metadata": {},
   "source": [
    "## 8. GraphFrames（圖計算）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95a9a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from graphframes import GraphFrame\n",
    "\n",
    "# 定義節點與邊\n",
    "vertices = spark.createDataFrame([(1, \"Alice\"), (2, \"Bob\")], [\"id\", \"name\"])\n",
    "edges = spark.createDataFrame([(1, 2)], [\"src\", \"dst\"])\n",
    "\n",
    "# 創建圖\n",
    "g = GraphFrame(vertices, edges)\n",
    "\n",
    "# 運行 PageRank 演算法\n",
    "result = g.pageRank(resetProbability=0.15, maxIter=10)\n",
    "result.vertices.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
