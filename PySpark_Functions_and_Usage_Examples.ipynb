{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03c5c478",
   "metadata": {},
   "source": [
    "# PySpark Functions and Usage Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111dd7d9",
   "metadata": {},
   "source": [
    "## 1. SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e1bc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"ExampleApp\").getOrCreate()\n",
    "\n",
    "# Get SparkContext\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Stop SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a13a8f",
   "metadata": {},
   "source": [
    "## 2. DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff31bcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create DataFrame\n",
    "data = [(1, \"Alice\", 29), (2, \"Bob\", 31)]\n",
    "columns = [\"id\", \"name\", \"age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Display data\n",
    "df.show()                 # Show the first 20 rows\n",
    "df.printSchema()          # Print the schema\n",
    "df.columns                # Get column names\n",
    "df.dtypes                 # Get column names and types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c24e8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Filter rows\n",
    "filtered_df = df.filter(df.age > 30)\n",
    "\n",
    "# Select columns\n",
    "selected_df = df.select(\"name\", \"age\")\n",
    "\n",
    "# Order by column\n",
    "sorted_df = df.orderBy(\"age\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444ffbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import avg, count\n",
    "\n",
    "# Aggregations\n",
    "df.groupBy(\"age\").count().show()\n",
    "df.agg(avg(\"age\").alias(\"average_age\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c40617e",
   "metadata": {},
   "source": [
    "## 3. SQL Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e810c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create temporary table\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# Execute SQL query\n",
    "result = spark.sql(\"SELECT name, age FROM people WHERE age > 30\")\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5fbb9c",
   "metadata": {},
   "source": [
    "## 4. RDD (Resilient Distributed Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13009db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create RDD\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "\n",
    "# Map and filter\n",
    "mapped_rdd = rdd.map(lambda x: x * 2)\n",
    "filtered_rdd = rdd.filter(lambda x: x > 3)\n",
    "\n",
    "# Collect results\n",
    "collected = filtered_rdd.collect()\n",
    "print(collected)  # [4, 5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a8674a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reduce\n",
    "sum_rdd = rdd.reduce(lambda x, y: x + y)\n",
    "\n",
    "# Aggregations\n",
    "count = rdd.count()\n",
    "max_value = rdd.max()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876437da",
   "metadata": {},
   "source": [
    "## 5. DataFrame Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f5826a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import col, lit, when\n",
    "\n",
    "# Column operations\n",
    "df = df.withColumn(\"new_column\", df.age * 2)\n",
    "\n",
    "# Conditional operations\n",
    "df = df.withColumn(\"is_adult\", when(col(\"age\") >= 18, lit(True)).otherwise(lit(False)))\n",
    "\n",
    "# String operations\n",
    "from pyspark.sql.functions import upper, lower\n",
    "df = df.withColumn(\"name_upper\", upper(col(\"name\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fead0f",
   "metadata": {},
   "source": [
    "## 6. MLlib (Machine Learning Library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da97ff28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Create feature vector\n",
    "assembler = VectorAssembler(inputCols=[\"age\"], outputCol=\"features\")\n",
    "data = assembler.transform(df)\n",
    "\n",
    "# Linear regression\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"age\")\n",
    "model = lr.fit(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f50fea",
   "metadata": {},
   "source": [
    "## 7. Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea68b31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create StreamingContext\n",
    "ssc = StreamingContext(sc, 1)  # Batch interval of 1 second\n",
    "\n",
    "# Create data stream\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "\n",
    "# Process data stream\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "word_counts = words.map(lambda word: (word, 1)).reduceByKey(lambda x, y: x + y)\n",
    "word_counts.pprint()\n",
    "\n",
    "# Start streaming\n",
    "ssc.start()\n",
    "ssc.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140ef8bb",
   "metadata": {},
   "source": [
    "## 8. GraphFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786ec571",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from graphframes import GraphFrame\n",
    "\n",
    "# Nodes and edges\n",
    "vertices = spark.createDataFrame([(1, \"Alice\"), (2, \"Bob\")], [\"id\", \"name\"])\n",
    "edges = spark.createDataFrame([(1, 2)], [\"src\", \"dst\"])\n",
    "\n",
    "# Create graph\n",
    "g = GraphFrame(vertices, edges)\n",
    "\n",
    "# Run PageRank\n",
    "result = g.pageRank(resetProbability=0.15, maxIter=10)\n",
    "result.vertices.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
